{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODNPestj7h_e"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#fce pro získání CLIP embeddingu\n",
        "def get_clip_embeddings(input_data, model, processor, input_type='text'):\n",
        "    if input_type == 'text':\n",
        "        inputs = processor(text=input_data, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        with torch.no_grad():\n",
        "            embeddings = model.get_text_features(**inputs)\n",
        "    elif input_type == 'image':\n",
        "        if isinstance(input_data, str):\n",
        "            image = Image.open(input_data)\n",
        "        inputs = processor(images=image, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            embeddings = model.get_image_features(**inputs)\n",
        "    return embeddings.numpy()\n",
        "\n",
        "# načtení modelu a processoru\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# načtení embeddingů obrázků (pro ukázku zmenšená sada 200 obrázků)\n",
        "image_dir = \"cesta k sample image\"\n",
        "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# počítání embeddingů pro všechny obrázky\n",
        "image_embeddings = []\n",
        "for image_path in image_paths:\n",
        "    embedding = get_clip_embeddings(image_path, model, processor, input_type='image')\n",
        "    image_embeddings.append(embedding)\n",
        "\n",
        "image_embeddings = np.vstack(image_embeddings)\n",
        "\n",
        "# fce pro Gradio aplikaci\n",
        "def find_similar_images(text_input):\n",
        "    # Získání embeddingu pro text\n",
        "    text_embedding = get_clip_embeddings(text_input, model, processor, input_type='text')\n",
        "\n",
        "    # Výpočet kosinové podobnosti mezi textem a obrázky\n",
        "    similarities = cosine_similarity(text_embedding, image_embeddings)\n",
        "\n",
        "    # Seřazení podle podobnosti\n",
        "    best_indices = np.argsort(similarities[0])[::-1][:4]\n",
        "\n",
        "    # Výběr nejlepších 4 obrázků\n",
        "    best_images = [image_paths[i] for i in best_indices]\n",
        "    return [Image.open(img) for img in best_images]\n",
        "\n",
        "# vytvoření Gradio rozhraní\n",
        "interface = gr.Interface(\n",
        "    fn=find_similar_images,\n",
        "    inputs=\"text\",\n",
        "    outputs=[gr.Image(type=\"pil\")] * 4,\n",
        "    title=\"Find Similar Images with CLIP\",\n",
        "    description=\"Enter a text prompt to find the most similar images.\"\n",
        ")\n",
        "\n",
        "# spuštění aplikace\n",
        "interface.launch() "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
